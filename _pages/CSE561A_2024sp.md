# Syllabus for CSE 561A: Large Language Models

| Date | Topic                                                                       | Readings                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Slides |
|------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|
| 1/16 | **Course Overview**                                                         |  [Distributed Representations of Words and Phrases and their Compositionality (Word2Vec)]()<br>[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)<br>[Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762) |        |
| 1/18 | **Language Model Architectures**                                            |  [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)<br>[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)<br>[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)<br>[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)  |        |
| 1/23 | **Large Language Model Training (I)**                                       |    [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)<br>[Cross-Task Generalization via Natural Language Crowdsourcing Instructions](https://arxiv.org/abs/2104.08773)<br>[LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)<br>[AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](https://arxiv.org/abs/2305.14387)       |        |
| 1/25 | **Large Language Model Training (II)**                                      |  [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)<br>[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)<br>[Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/abs/2306.01693)<br>[Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2307.15217)    |        |
| 1/30 | **Parameter Efficient Fine-Tuning**                                         | [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)<br>[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)<br>[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)<br>[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)   |        |                                                                                                                                            
|      | -----**Student Presentation Starts**-----                                   |                                                                                                       |        |       
| 2/1  |  **Prompting and In-context Learning**                                       |   [Language Models are Few-Shot Learners (GPT-3)]()<br>[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)<br>[Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)<br>[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)  |        |
| 2/6  | **Language Model Reasoning (I)**                                                | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)<br>[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf)<br>[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)<br>[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) |        |
| 2/8  |  **Language Model Reasoning (II)**  | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)<br>[Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)<br>[Progressive-Hint Prompting Improves Reasoning in Large Language Models](https://arxiv.org/abs/2304.09797)<br>[Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001)  |        |
| 2/13 | **Language Model Calibration/Uncertainty**                                  |  [How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering](https://aclanthology.org/2021.tacl-1.57.pdf)<br>[Teaching models to express their uncertainty in words](https://arxiv.org/abs/2205.14334)<br>[Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models](https://arxiv.org/abs/2302.13439)<br>[Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation](https://arxiv.org/abs/2302.09664)          
| 2/15 | **Retrieval Augmentation**                                                  | [Generalization through Memorization: Nearest Neighbor Language Models](https://openreview.net/forum?id=HklBjCEKvH)<br>[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)<br>[Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)<br>[Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349) | |
|      | -----**Project Proposal Deadline: Before 1st Class in Week 6**-----         |            |        |
| 2/20 | **Decoding**                                                   |    [Contrastive Decoding: Open-ended Text Generation as Optimization](https://arxiv.org/abs/2210.15097)<br>[Don't throw away your value model! Making PPO even better via Value-Guided Monte-Carlo Tree Search decoding](https://arxiv.org/abs/2309.15028)<br>[Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://arxiv.org/abs/2307.15337)<br>[Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)    |        |
| 2/22 | Course Cancelled Due to Business Travel                                     |   |        |
| date | **Code Language Models**                                                    |   [InCoder: A Generative Model for Code Infilling and Synthesis](https://arxiv.org/abs/2204.05999)<br>[Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)<br>[Teaching Large Language Models to Self-Debug](https://arxiv.org/abs/2304.05128)<br>[LEVER: Learning to Verify Language-to-Code Generation with Execution](https://arxiv.org/abs/2302.08468)    |        |
| date | **Multimodal Language Models**                                         |   [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)<br>[VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)<br>[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)<br>[NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519)  |        |
| date | **Language Models as Agents**                                               | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)<br>[Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)<br>[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)<br>[ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014)<br>[Reflexion: Language Agents with Verbal Reinforcement Learning]() |        |
|date | Project Discussion (Tentative) | |
| date  | **Evaluation of Language Models**                                        |   [Proving Test Set Contamination in Black Box Language Models]()<br>[Holistic Evaluation of Language Models]()<br>[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena]()<br>[FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)  |        |
| date | **Long-Context Language Models**                                            |  [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)<br> [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)<br>[LongNet: Scaling Transformers to 1B Tokens](https://arxiv.org/abs/2307.02486)<br>[Memorizing Transformers](https://arxiv.org/abs/2203.08913)|        |
|      | -----**Project Mid-Term Report Deadline: Before 1st Class in Week 10**----- |    |        |
| date | **Dynamic Architecture and Knowledge**                                                           | [Depth-Adaptive Transformer](https://arxiv.org/abs/2212.01349)<br>[DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://aclanthology.org/2020.acl-main.204/)<br>[How is ChatGPT's behavior changing over time?](https://arxiv.org/abs/2307.09009)<br>[Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401)  |        |
|date| **Language Model Bias**                                                     |  [Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints](https://arxiv.org/abs/1707.09457)<br>[Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP](https://arxiv.org/abs/2103.00453)<br>[Whose Opinions Do Language Models Reflect?](https://arxiv.org/abs/2303.17548)<br>[Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286) | |
| date | **Privacy**                                                        | [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)<br>[Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/abs/2110.05679)<br>[Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646)[SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://arxiv.org/abs/2308.04430)  |        |
| date  | **Security**                                                        | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)<br>[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)<br>[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)<br>[GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://arxiv.org/abs/2308.06463)  |        |
| date  | **Explorations of Large Language Models**                         |  [Weak-To-Strong Generalization:Eliciting Strong Capabilities With Weak Supervision](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)<br>[Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/abs/2212.14052)<br>[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)<br>[When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities](https://arxiv.org/abs/2307.16376)   |        |
| date | **Guest Lecture**                                                           |    |        |
| date | **Guest Lecture**                                                           |     |        |
|      | -----**Project Presentation Deadline: 4/17 11:59pm**-----                   |     |        |
| 4/18 | **Final Project Presentation I**                                            |     |        |
| 4/23 | **Final Project Presentation II**                                           |    |        |
| 4/25 | **Final Project Presentation III**                                          |  |        |
|      | -----**Project Final Report Deadline: 4/26 11:59pm**-----                    |   |        |
