---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am an Assistant Professor in Computer Science Engineering (CSE) Department, Washington University in St. Louis (WashU). I received my Ph.D. degree in Computer Science Department, UIUC, advised by Prof. [Jiawei Han](http://hanj.cs.illinois.edu). After that, I visited UW as a researcher and worked with Prof. [Hanna Hajishirzi](https://homes.cs.washington.edu/~hannaneh/). Prior to UIUC, I received my Bachelor Degree in Electronic Engineering in Tsinghua University in 2018. My research interest broadly lies in the intersection of natural language processing and machine learning, and I am especially interested in understanding the properties of language models as well as improving their trustworthiness and efficiency.

I am looking for PhD students for 25'Fall and interns year-round! If you are interested in working with me, please fill in [this form](https://docs.google.com/forms/d/e/1FAIpQLSeFDUgS0Q6CiH2-SjztPRwPP4DxCED5THhKdTOgIP3fh4LSTw/viewform) . While submitting the form is already sufficient, you are also welcome to reach out to me via email. I will read every email but please understand that I might not be able to respond to each of them due to time constraints.


Recent Research Interests
======
* <em>**Large Language Model Factuality and Calibration**</em>: I build methods for integrating factual knowledge within language models, including graphs([1](https://arxiv.org/abs/2407.09709)), ontologies([1](https://arxiv.org/abs/2310.07795),[2](https://arxiv.org/abs/2010.06714),[3](https://arxiv.org/abs/2007.09536)), entities([1](https://arxiv.org/abs/2206.13746),[2](https://arxiv.org/abs/2012.14978)), and applying them for downstream tasks([1](https://arxiv.org/abs/2110.08845)) in low-resource setting. Recently, I am working on improving language model reliability to align their confidence with performance.
* <em>**Large Language Model Reasoning**</em>: I investigate how large language model generations can self-improve their reasoning abilities ([unsupervised self-improving reasoning](https://arxiv.org/abs/2210.11610), [weakly-supervised reasoning](https://arxiv.org/abs/2405.04086)).
* <em>**Training and Inference Efficiency of Large Language Models**</em>: I work on improving language model training and inference efficiency ([multi-task efficient inference](https://arxiv.org/abs/2205.10744)).
* <em>**Data Efficiency for Language Model Training**</em>: I study how to better fine-tune language model with limited training data, including data self-generation([1](https://arxiv.org/abs/2211.03044),[2](https://arxiv.org/abs/2202.04538),[3](https://aclanthology.org/2020.emnlp-main.724/)), denoising [distant-supervision](https://arxiv.org/abs/2109.05003) and integrating [metadata](https://arxiv.org/abs/2005.00624).
* <em>**Representation Learning**</em>: I study how the text embedding space could be regularized in different circumstances ([category-based](https://arxiv.org/abs/1908.07162), [joint-categories learning](https://arxiv.org/abs/2010.06705), [contextualized](https://arxiv.org/abs/2202.04582), etc.)
  


Honors and Awards
======
Microsoft Research PhD Fellowship  2021-2023  
C.W. Gear Outstanding Graduate Award
Chirag Foundation Graduate Fellowship  
Outstanding Graduates, Tsinghua University  2018  
Academic Excellence Scholarship, Tsinghua University  2015-2017  
China National Scholarship (Top 1%)  2016  
Samsung Scholarship  2015  

 
Education
======
* University of Illinois, Urbana-Champaign (2018.08-2023.12)  
  Ph.D. in Computer Science  
  Research Interest: Natural Language Processing, Data Mining
  Advisor: Prof. [Jiawei Han](http://hanj.cs.illinois.edu)  

* Tsinghua University (2014.08-2018.07)  
  B. Eng. in Electronic Engineering  
  Advisor: Prof. [Yong Li](http://fi.ee.tsinghua.edu.cn/~liyong/)  

* University of California, Los Angeles (2017.07-2017.09)  
  CSST summer program  
  Advisor: Prof. [Wei Wang](http://web.cs.ucla.edu/~weiwang/)  


Experience
======
* Google Core-ML (2022.06-2022.09)  
  Research Intern  
  Host: Dr. [Le Hou](https://scholar.google.com/citations?user=kQ0HeQIAAAAJ&hl=en) and Dr. [Yuexin Wu](https://scholar.google.com/citations?user=sd0nprMAAAAJ&hl=en)

* Google Research (2021.05-2021.08)  
  Research Intern  
  Host: Dr. [Tianqi Liu](https://scholar.google.com/citations?user=pUKhiMIAAAAJ&hl=en) and Dr. [Jialu Liu](https://jialu.info/)

* Microsoft Research Redmond (2020.06-2020.09)  
  Research Intern  
  Host: Dr. [Chunyuan Li](http://chunyuan.li/) and Krishan Subudhi  





