---
layout: archive	
title: " CSE 4061: Text Mining (2026 Spring)"
permalink: /CSE4061_2026sp/
author_profile: true	  
---


<h2 id="course-overview">Course Overview</h2>
<p>This is an advanced research-oriented course that teaches fundamental techniques of text mining and natural language processing. It is a rapidly evolving field at the intersection of natural language processing and machine learning. Students will gain both in-depth knowledge of fundamental concepts and hands-on experience in practical applications.<br>Pre-requisites: Students are expected to understand concepts in machine learning (CSE 4107/5107)</p>


<h2 id="ta">Teaching Assistants</h2>
<p><strong>Jinyuan Li</strong> (ljinyuan@wustl.edu)<br>
<strong>Ryan Zhang</strong> (yongyan@wustl.edu)</p>

<h2 id="course-grading">Course Grading</h2>
<ul>
<li>10% Class Participation<ul>
<li>Regular class participation and discussion (10%)</li>
</ul>
</li>
<li>60% Programming Assignments
<ul>
<li>4 programming assignments, each account for 15%</li>
<li>5 days of grace period in total for all assignments</li>
<li>After grace period is used up, late assignments receive 0 score</li>
</ul>
</li>
<li>30% Final Project (Group-Based, 2-3 people)
<ul>
<li>5% Project Proposal (Due: 2/3, 11:59PM)</li>
<li>5% Mid-term Report (Due: 3/5, 11:59PM)</li>
<li>10% Final Course Presentation (Due: 4/20, 11:59PM)</li>
<li>We will use two lectures for project presentation: 4/21, 4/23
</li>
<li>10% Final Project Report (Due: 5/1, 11:59PM)</li>
</ul>
</li>
</ul>

<h2 id="final-project-2-3-students-per-group-">Final Project (2-3 students per group)</h2>
<p>Project Requirement: Demonstrate that you are able to apply the knowledge and techniques learned from this course. The project requires more complex implementation than the programming assignments. Topics include but not limited to: </p>
<ol>
<li>Investigate word embeddings and sentence embeddings for text classification problems.</li>
<li>Train a medium-sized language model (e.g., BERT, GPT-2) for tasks that you are interested in.</li>
<ul><li><a href="https://huggingface.co/models">https://huggingface.co/models</a></li></ul>
<li>Do inference on large language models (white box models such as LLaMA models, or black box models such as GPT-4, CLAUDE, etc.) to solve some type of complex problems, and analyze their limitations.</li>
<ul><li><a href="https://platform.openai.com/docs/introduction">https://platform.openai.com/docs/introduction</a></li>
<li><a href="https://docs.anthropic.com/claude/reference/getting-started-with-the-api">https://docs.anthropic.com/claude/reference/getting-started-with-the-api</a></li>
</ul>
<li>Create benchmark for new and challenging tasks and test it with SoTA models.</li>
<ul></ul>
</ol>

<p>Project Presentation Date: 4/15 and 4/17, 2025. You will need to signup for a time slot near the end of the semester. Presentation length will be 10-15 minutes depending on the number of groups.
</p>

<h2 id="office-hour">Office Hour</h2>
<p>Instructor Office Hour: Thursday 11am - 12pm at McKelvey Hall 2010E</p>
<p>TA Office Hour: Thursday 1-2pm at TBD (Jinyuan Li)</p>
<p>TA Office Hour: TBD at TBD (Ryan Zhang)</p>

<h2 id="course-policies">Course Policies</h2>
<ul>
<li>LLM Usage Policy:
<ul>
<li>It is fine to collaborate with LLMs for coding assignments and refining your reports. However, directly using LLM generated outputs without manual check results in 0 score of the assignment.</li>
</ul>
</li>
<li>Extra Credit:
<ul>
<li>Students who first correctly answers technical questions (excluding assignment questions) raised by other students on Piazza will get 1 bonus point each time, up to 3 points in total.</li>
</ul>
</li>
</ul>


<h2 id="syllabus-the-dates-of-the-courses-are-tentative-due-to-guest-lectures-">Syllabus (The content of each class is tentative.)</h2>

<table>
    <tr>
        <td><strong>Week</strong></td>
        <td><strong>Date</strong></td>
        <td><strong>Topic</strong></td>
        <td><strong>Assignment</strong></td>
    </tr>
    <tr>
        <td rowspan="2">Week 1</td>
        <td>01/13</td>
        <td>Course Overview</td>
        <td></td>
    </tr>
    <tr>
        <td>01/15</td>
        <td>N-gram Models</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 2</td>
        <td>01/20</td>
        <td>Bag of Words, TF-IDF</td>
        <td></td>
    </tr>
    <tr>
        <td>01/22</td>
        <td>Word Representations and Neural Word Embeddings</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 3</td>
        <td>01/27</td>
        <td>Neural Word Embeddings (Cont'd)</td>
        <td></td>
    </tr>
    <tr>
        <td>01/29</td>
        <td>Document Representations</td>
        <td>HW1 Out</td>
    </tr>
    <tr>
        <td rowspan="2">Week 4</td>
        <td>02/03</td>
        <td>Neural Sequence Modeling (RNN, LSTM)</td>
        <td>Proposal Due</td>
    </tr>
    <tr>
        <td>02/05</td>
        <td>Neural Sequence Modeling and Self Attention</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 5</td>
        <td>02/10</td>
        <td>Transformer architectures</td>
        <td>HW1 Due</td>
    </tr>
    <tr>
        <td>02/12</td>
        <td>LLM Pre-training</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 6</td>
        <td>02/17</td>
        <td>Text Mining Applications: Sentiment Analysis</td>
        <td>HW2 Out</td>
    </tr>
    <tr>
        <td>02/19</td>
        <td>Text Mining Applications: Information Extraction</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 7</td>
        <td>02/24</td>
        <td>Large Language Models: Pre-training and Scaling</td>
        <td></td>
    </tr>
    <tr>
        <td>02/26</td>
        <td>Advanced LLM reasoning (I): Prompting</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 8</td>
        <td>03/03</td>
        <td>Instruction Tuning</td>
        <td>HW2 Due</td>
    </tr>
    <tr>
        <td>03/05</td>
        <td>Reinforcement Learning with Human Feedback</td>
        <td>HW3 Out, Mid-Term Report Due</td>
    </tr>
    <tr>
        <td colspan="4" align="center"><strong>Spring Break</strong></td>
    </tr>
    <tr>
        <td rowspan="2">Week 10</td>
        <td>03/17</td>
        <td>Advanced LLM reasoning (II): Reinforcement Learning with Verifiable Rewards</td>
        <td></td>
    </tr>
    <tr>
        <td>03/19</td>
        <td>Advanced LLM reasoning (III): Self-Training and Routing</td>
        <td>HW3 Due</td>
    </tr>
    <tr>
        <td rowspan="2">Week 11</td>
        <td>03/24</td>
        <td>Language Model Factuality</td>
        <td>HW4 Out</td>
    </tr>
    <tr>
        <td>03/26</td>
        <td>LLM Training Efficiency</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 12</td>
        <td>03/31</td>
        <td>LLM Inference Efficiency</td>
        <td></td>
    </tr>
    <tr>
        <td>04/02</td>
        <td>LLM Multi-modality</td>
        <td>HW4 Due</td>
    </tr>
    <tr>
        <td rowspan="2">Week 13</td>
        <td>04/07</td>
        <td>LLM Applications: Retrieval-Augmented Generation</td>
        <td></td>
    </tr>
    <tr>
        <td>04/09</td>
        <td>LLM Applications: Agents</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 14</td>
        <td>04/14</td>
        <td>Future Directions</td>
        <td></td>
    </tr>
    <tr>
        <td>04/16</td>
        <td>TBD</td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2">Week 15</td>
        <td>04/21</td>
        <td>Final Project Presentations</td>
        <td></td>
    </tr>
    <tr>
        <td>04/23</td>
        <td>Final Project Presentations</td>
        <td></td>
    </tr>
</table>
