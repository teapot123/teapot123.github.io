---
layout: archive	
title: " CSE 561A: Large Language Models"
permalink: /CSE561A_2024sp/
author_profile: true	  
---


## Course Overview
This is an advanced research-oriented course that teaches fundamentals of Large Language Models (language model architecture and training framework) as well as Large Language Model capabilities, applications and issues. We will be teaching and discussing state-of-the-art papers about large language models.  
Pre-requisites: Students are expected to understand concepts in machine learning (CSE 417T/517A)  
Canvas: [https://wustl.instructure.com/courses/129974](https://wustl.instructure.com/courses/129974)
Piazza: [https://piazza.com/class/lsf6np5hzai4rs](https://piazza.com/class/lsf6np5hzai4rs)



## Course Grading
- 15% Class Participation
  - Regular class participation and discussion (10%)
  - Preview question submissions (5%)
- 30% Class Presentation
- 55% Final Project 
  - 10% Project Proposal
  - 10% Mid-term Report
  - 10% Final Course Presentation (Group-based)
  - 5% Feedbacks for other groups’ final project presentations 
  - 20% Final Project Report

## Class Presentation
Grading Criteria:
- Good Preparation: Whether the slides are sent over by the given deadline for the instructors to give feedback
  - For Tuesday classes, send over your slides before the previous Friday 12:00PM
  - For Thursday classes, send over your slides before the previous Monday 12:00PM
- Completeness: Whether the presentation covers the background and major contribution of the listed papers, and is delivered within the required timeframe
- Clarity: Whether the presenter clearly convey the information from their slides
- Q&A: If there are any raised questions from the audiences, whether the presenters can handle the questions properly

Each student is also required to submit a preview question for a paper one day before the presentation for 3 times (need to be on 3 different classes, and not the date that you present). You are also encouraged to raise that question in class.
Preview questions cannot be simple ones like "what is the aim of the paper?" or "what is the difference between this method and traditional method in nlp?"

## Final Project (2-3 students per group)
Project Requirement:
There are typically two types of projects.
1. Designing a novel algorithm to train a medium-sized language model: BERT, GPT-2 for problems that you are interested in.
- [https://huggingface.co/models](https://huggingface.co/models)
2. Designing a novel algorithm to do inference on large language models (white box models such as LLaMA2 models, or black box models such as GPT-4, CLAUDE, etc.) to solve some type of complex problems, and analyze their limitations. (We may not be able to reimburse for the API costs, so you can choose to use free APIs such as CLAUDE)
- [https://platform.openai.com/docs/introduction](https://platform.openai.com/docs/introduction)
- [https://docs.anthropic.com/claude/reference/getting-started-with-the-api](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)

Project Presentation:
Date: 4/18, 4/23, 4/25. You will need to signup for a time slot near the end of the semester.
Students will need to submit feedback scores for other groups’ presentation (through Google Form).


## Office Hour

Our office hour will be on-demand ones: If you find yourself needing to discuss course materials or have questions at any point, feel free to send an email requesting an office hour. Based on these requests, we will organize time slots for students to schedule appointments.


## Syllabus (The dates of the courses are tentative due to guest lectures.)


| Date | Topic                                                                       | Readings                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Slides |
|------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|
| 1/16 | **Course Overview**                                                         |  [Distributed Representations of Words and Phrases and their Compositionality (Word2Vec)]()<br>[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)<br>[Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762) |  [Slides](/files/CSE561A_slides/Lecture_1.pdf)      |
| 1/18 | **Language Model Architectures**                                            |  [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)<br>[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)<br>[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)<br>[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)  |  [Slides](/files/CSE561A_slides/Lecture_2.pdf)      |
| 1/23 | **Large Language Model Training (I)**                                       |    [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)<br>[Cross-Task Generalization via Natural Language Crowdsourcing Instructions](https://arxiv.org/abs/2104.08773)<br>[LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)<br>[Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)<br>[How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)       |  [Slides](/files/CSE561A_slides/Lecture_3.pdf)      |
| 1/25 | **Large Language Model Training (II)**                                      |  [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)<br>[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)<br>[Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/abs/2306.01693)<br>[Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2307.15217)    |   [Slides](/files/CSE561A_slides/Lecture_4.pdf)     |
| 1/30 | **Parameter Efficient Fine-Tuning**                                         | [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)<br>[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)<br>[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)<br>[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)   |  [Slides](/files/CSE561A_slides/Lecture_5.pdf)      |                                                                                                                                            
|      | -----**Student Presentation Starts**-----                                   |                                                                                                       |        |       
| 2/1  |  **Prompting and In-context Learning**                                       |   [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)<br>[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)<br>[Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)<br>[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)  |  [Slides_1](/files/CSE561A_slides/Lecture_6_1.pdf)<br>[Slides_2](/files/CSE561A_slides/Lecture_6_2.pdf)      |
| 2/6  | **Language Model Reasoning (I)**                                                | [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)<br>[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf)<br>[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)<br>[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) |  [Slides_1](/files/CSE561A_slides/Lecture_7_1.pptx)<br>[Slides_2](/files/CSE561A_slides/Lecture_7_2.pdf)      |
| 2/8  |  **Language Model Reasoning (II)**  | [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)<br>[Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)<br>[Progressive-Hint Prompting Improves Reasoning in Large Language Models](https://arxiv.org/abs/2304.09797)<br>[Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001)  |  [Slides_1](/files/CSE561A_slides/Lecture_8_1.pdf)<br>[Slides_2](/files/CSE561A_slides/Lecture_8_2.pdf)      |
| 2/13 | **Language Model Calibration/Uncertainty**                                  |  [How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering](https://aclanthology.org/2021.tacl-1.57.pdf)<br>[Teaching models to express their uncertainty in words](https://arxiv.org/abs/2205.14334)<br>[Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models](https://arxiv.org/abs/2302.13439)<br>[Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation](https://arxiv.org/abs/2302.09664)   | [Slides_1](/files/CSE561A_slides/Lecture_9_1.pdf)<br>[Slides_2](/files/CSE561A_slides/Lecture_9_2.pdf) |       
| 2/15 | **Retrieval Augmentation and Parametric Knowledge**                                                  | [Generalization through Memorization: Nearest Neighbor Language Models](https://openreview.net/forum?id=HklBjCEKvH)<br>[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)<br>[Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066)<br>[How Much Knowledge Can You Pack Into the Parameters of a Language Model?](https://arxiv.org/abs/2002.08910) | [Slides_1](/files/CSE561A_slides/Lecture_10_1.pdf)<br>[Slides_2](/files/CSE561A_slides/Lecture_10_2.pdf) |
|      | -----**Project Proposal Deadline: 2/19 11:59pm**-----         |            |        |
| 2/20 | **Decoding**                                                   |    [Contrastive Decoding: Open-ended Text Generation as Optimization](https://arxiv.org/abs/2210.15097)<br>[Don't throw away your value model! Making PPO even better via Value-Guided Monte-Carlo Tree Search decoding](https://arxiv.org/abs/2309.15028)    |        |
| 2/22 | Course Cancelled Due to Business Travel                                     |   |        |
| 2/27 | **Code Language Models**                                                    |   [InCoder: A Generative Model for Code Infilling and Synthesis](https://arxiv.org/abs/2204.05999)<br>[Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)<br>[Teaching Large Language Models to Self-Debug](https://arxiv.org/abs/2304.05128)<br>[LEVER: Learning to Verify Language-to-Code Generation with Execution](https://arxiv.org/abs/2302.08468)    |        |
| 2/29 | **Multimodal Language Models**                                         |   [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)<br>[VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175)<br>[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)<br>[NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519)  |        |
| 3/5 | **Language Models as Agents**                                               | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)<br>[Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)<br>[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)<br>[ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014)|        |
| 3/7  | **Evaluation of Language Models**                                        |   [Proving Test Set Contamination in Black Box Language Models](https://arxiv.org/abs/2310.17623)<br>[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)  |        |
| | Spring Break | |
|      | -----**Project Mid-Term Report Deadline: 3/18 11:59pm**----- |    |        |
| 3/19 | **Long-Context Language Models**                                            |  [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)<br> [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)<br>[LongNet: Scaling Transformers to 1B Tokens](https://arxiv.org/abs/2307.02486)<br>[Memorizing Transformers](https://arxiv.org/abs/2203.08913)|        |
| 3/21 | **Dynamic Architecture and Knowledge**                                                           | [Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073)<br>[DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://aclanthology.org/2020.acl-main.204/)<br>[How is ChatGPT's behavior changing over time?](https://arxiv.org/abs/2307.09009)<br>[Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401)  |        |
|3/26| **Language Model Bias**                                                     |  [Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints](https://arxiv.org/abs/1707.09457)<br>[Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP](https://arxiv.org/abs/2103.00453)<br>[Whose Opinions Do Language Models Reflect?](https://arxiv.org/abs/2303.17548)<br>[Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286) | |
| 3/28 | **Privacy**                                                        | [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)<br>[Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/abs/2110.05679)<br>[Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646)<br>[SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://arxiv.org/abs/2308.04430)  |        |
| 4/2  | **Security**                                                        | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)<br>[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)<br>[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)<br>[GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://arxiv.org/abs/2308.06463)  |        |
| 4/4  | **Explorations of Large Language Models**                         |  [Weak-To-Strong Generalization:Eliciting Strong Capabilities With Weak Supervision](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)<br>[Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/abs/2212.14052)<br>[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)<br>[When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities](https://arxiv.org/abs/2307.16376)   |        |
| 4/9 | **Guest Lecture (Tentative)**                                                           |    |        |
| 4/11 | **Guest Lecture (Tentative)**                                                           |     |        |
| 4/16 | **Guest Lecture (Tentative)**                                                           |     |        |
|      | -----**Project Presentation Deadline: 4/17 11:59pm**-----                   |     |        |
| 4/18 | **Final Project Presentation I**                                            |     |        |
| 4/23 | **Final Project Presentation II**                                           |    |        |
| 4/25 | **Final Project Presentation III**                                          |  |        |
|      | -----**Project Final Report Deadline: 4/26 11:59pm**-----                    |   |        |
